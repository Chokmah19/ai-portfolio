# ğŸ§¹ 03ï½œæ–‡å­—å‰è™•ç†èˆ‡è³‡æ–™æº–å‚™



## ğŸ§© 1. clean_text å‡½å¼è¨­è¨ˆé‚è¼¯

ç‚ºäº†è®“æ¨¡å‹èƒ½æ­£ç¢ºå­¸ç¿’æ–‡å­—è³‡æ–™ï¼Œæˆ‘å€‘å»ºç«‹äº†ä¸€å€‹ `clean_text(text)` å‡½å¼ï¼Œåˆ†åˆ¥è™•ç†è¼¸å…¥æ–‡å­—ä¸­çš„é›œè¨Šã€æ ¼å¼èˆ‡ç„¡æ„ç¾©è©ï¼Œæ­¥é©Ÿå¦‚ä¸‹ã€‚

---

### æ­¥é©Ÿèªªæ˜ï¼š

1. ğŸ§½ **ç§»é™¤éæ–‡å­—å­—å…ƒ**  
   ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ `re.sub(r"[^\w\s]", "", text)`  
   âœ æ¸…é™¤æ¨™é»ç¬¦è™Ÿã€emojiã€ç‰¹æ®Šç¬¦è™Ÿï¼Œåªä¿ç•™å­—æ¯ã€æ•¸å­—èˆ‡ç©ºæ ¼  

2. ğŸ”¡ **çµ±ä¸€æˆå°å¯«**  
   ä½¿ç”¨ `.lower()` æ–¹æ³•  
   âœ é¿å… "Love" èˆ‡ "love" è¢«æ¨¡å‹è¦–ç‚ºä¸åŒè©  

3. âœ‚ï¸ **åˆ†è©ï¼ˆtokenizationï¼‰**  
   ä½¿ç”¨ `.split()`  
   âœ æŠŠå¥å­åˆ‡æˆå–®å­—çµ„æˆçš„æ¸…å–®  

4. ğŸš« **ç§»é™¤ stop words**  
   ä½¿ç”¨ `sklearn.feature_extraction.text.ENGLISH_STOP_WORDS`  
   âœ éæ¿¾æ‰å¸¸è¦‹ä½†èªæ„è–„å¼±çš„è©ï¼ˆå¦‚ "the", "is", "and"ï¼‰

---

## ğŸ§ª clean_text å‡½å¼ç¨‹å¼ç¢¼

```python
import re
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

stop_words = ENGLISH_STOP_WORDS

def clean_text(text):
    cleaned = re.sub(r"[^\w\s]", "", text)
    lowercase = cleaned.lower()
    words = lowercase.split()
    filtered_words = [word for word in words if word not in stop_words]
    return filtered_words
```

### ğŸ§  æˆ‘çš„ç–‘å•èˆ‡å­¸ç¿’ç´€éŒ„

#### â“æˆ‘å•ï¼š
- ã€Œç‚ºä»€éº¼æ–‡å­—è³‡æ–™éœ€è¦åšå‰è™•ç†ï¼Ÿã€
- ã€Œæ€éº¼æŒ‘å‡ºé‡é»è©å½™ï¼Ÿï¼ˆæ€éº¼æ±ºå®šå“ªäº›è©è©²ä¿ç•™ï¼‰ã€
- ã€Œlist comprehension æ˜¯ä»€éº¼ï¼Ÿã€
- ã€Œç‚ºä»€éº¼ä¸èƒ½ç›´æ¥è®“æ¨¡å‹åƒæ–‡å­—ï¼Œè€Œè¦è½‰æˆæ•¸å­—ï¼Ÿã€
- ã€Œç‚ºä»€éº¼è½‰ list âœ string è¦ç”¨ joinï¼Œè€Œä¸èƒ½ç”¨ df['col'].join('')ï¼Ÿã€

#### âœ… æˆ‘å­¸åˆ°ï¼š
- å‰è™•ç†å¯ä»¥çµ±ä¸€æ ¼å¼ã€æ¿¾æ‰å™ªéŸ³è©ã€å¹«åŠ©æ¨¡å‹æ›´æœ‰æ•ˆå­¸ç¿’
- å¯ä»¥ç”¨ sklearn å…§å»ºçš„ stop words æ¸…å–®æ¿¾æ‰å¸¸è¦‹ç„¡æ„ç¾©è©
- list comprehension æ˜¯ç”¨ä¸€è¡Œç°¡æ½”çš„èªæ³•ä¾†éæ¿¾æˆ–è½‰æ› list
- æ¨¡å‹åªèƒ½åšæ•¸å­¸é‹ç®—ï¼Œæ‰€ä»¥å¿…é ˆæŠŠæ–‡å­—è½‰æˆæ•¸å­—å‘é‡ï¼ˆæ‰èƒ½å­¸ç¿’ã€é‹ç®—ï¼‰
- df['col'].join('') éŒ¯æ˜¯å› ç‚º .join() åªèƒ½ç”¨åœ¨å–®ä¸€ listï¼Œä¸æ˜¯æ•´å€‹æ¬„ä½ Series

#### ğŸ“ é™„åŠ å‚™è¨»
ç‚ºäº†è®“ CountVectorizer è™•ç†æ–‡å­—ï¼Œæˆ‘å€‘å°‡æ¯ç­† token æ¸…å–®è½‰æˆå­—ä¸²ï¼š

```python
df['clean_text'] = df['clean_tokens'].apply(lambda tokens: ' '.join(tokens))
```
- æˆ‘æ›¾è©¦è‘—å¯«æˆ df['clean_tokens'].join('')ï¼Œä½†éŒ¯èª¤åŸå› æ˜¯ .join() è¦ç”¨åœ¨ listï¼Œä¸æ˜¯ Seriesã€‚

ğŸ”œ ä¸‹ä¸€æ­¥
æˆ‘æ¥ä¸‹ä¾†æœƒä½¿ç”¨ CountVectorizer æŠŠ clean_text æ¬„ä½è½‰æˆæ¨¡å‹å¯ç”¨çš„æ•¸å­—å‘é‡æ ¼å¼ï¼Œä¸¦æ§åˆ¶è©å½™æ•¸é‡ä»¥é¿å…éåº¦æ“´å¼µã€‚


## ğŸ§® 2. è©å½™å‘é‡åŒ–ï¼ˆä½¿ç”¨ CountVectorizerï¼‰

æ–‡å­—ç¶“éæ¸…æ´—èˆ‡æ–·è©å¾Œï¼Œç„¡æ³•ç›´æ¥æä¾›çµ¦æ©Ÿå™¨å­¸ç¿’æ¨¡å‹ä½¿ç”¨ï¼Œå¿…é ˆé€²ä¸€æ­¥è½‰æ›ç‚ºæ•¸å€¼æ ¼å¼ï¼ˆå‘é‡ï¼‰ã€‚é€™ä¸€æ®µæˆ‘å€‘ä½¿ç”¨ `CountVectorizer` å°‡æ¯ç­†è©•è«–è½‰æ›ç‚ºã€Œè©è¢‹æ¨¡å‹ã€çš„å‘é‡ã€‚

---

### ğŸ§  æˆ‘çš„ç–‘å•èˆ‡å­¸ç¿’ç´€éŒ„

#### â“æˆ‘å•ï¼š
- ã€Œç‚ºä»€éº¼ä¸èƒ½ç›´æ¥è®“æ¨¡å‹åƒæ–‡å­—ï¼Ÿã€
- ã€Œå‘é‡åŒ–ä¹‹å¾Œçš„è³‡æ–™æ¯åˆ—æ¯æ¬„æ˜¯ä»€éº¼ï¼Ÿã€
- ã€Œç‚ºä»€éº¼è¦é™åˆ¶è©å½™è¡¨å¤§å°ï¼Ÿã€

#### âœ… æˆ‘å­¸åˆ°ï¼š
- æ¨¡å‹åªèƒ½åšæ•¸å­¸é‹ç®—ï¼Œç„¡æ³•ç†è§£æ–‡å­—æœ¬èº«
- å‘é‡åŒ–å¾Œçš„æ¯åˆ—æ˜¯ä¸€ç­†è©•è«–ï¼Œæ¯æ¬„æ˜¯ä¸€å€‹è©ï¼ˆç‰¹å¾µï¼‰ï¼Œå€¼ç‚ºå‡ºç¾æ¬¡æ•¸
- å¦‚æœè©å½™è¡¨å¤ªå¤§ï¼ˆä¸€é–‹å§‹è¶…é 18 è¬æ¬„ï¼‰æœƒé€ æˆè¨“ç·´éæ…¢ã€éæ“¬åˆï¼Œç”šè‡³åŒ…å«å™ªéŸ³
- `max_features=10000` å¯åªä¿ç•™å¸¸è¦‹å‰ 10,000 å€‹è©

---

### âœ… é‡é»ç¨‹å¼ç¢¼

```python
from sklearn.feature_extraction.text import CountVectorizer

# å°‡æ–·è©å¾Œçš„æ¸…å–®è½‰ç‚ºæ–‡å­—ï¼ˆçµ¦ Vectorizer ç”¨ï¼‰
df['clean_text'] = df['clean_tokens'].apply(lambda tokens: ' '.join(tokens))

# å»ºç«‹å‘é‡å™¨ï¼Œé™åˆ¶æœ€å¤š 10,000 å€‹è©å½™
vectorizer = CountVectorizer(max_features=10000)
X = vectorizer.fit_transform(df['clean_text'])

print(X.shape)  # âœ (50000, 10000)
```

## â„¹ï¸ 3. é¿å…è³‡æ–™å¤–æ´©ï¼ˆData Leakageï¼‰

å¦‚æœåœ¨åˆ‡åˆ†è³‡æ–™é›†ä¹‹å‰å°±ç”¨å…¨éƒ¨è³‡æ–™ä¾† `fit` å‘é‡å™¨ï¼Œæ¸¬è©¦é›†çš„è©å½™è³‡è¨Šæœƒæå‰è¢«æ¨¡å‹ã€Œå·çœ‹ã€ï¼Œå°è‡´æ¸¬è©¦çµæœéæ–¼æ¨‚è§€ï¼Œé€™å°±æ˜¯è³‡æ–™å¤–æ´©ã€‚

---

### ğŸ—’ï¸ æ­£ç¢ºæµç¨‹

1. å…ˆç”¨ `train_test_split` åˆ‡æˆè¨“ç·´é›†èˆ‡æ¸¬è©¦é›†ï¼ˆåŒæ™‚åˆ‡æ–‡å­—èˆ‡æ¨™ç±¤ï¼‰ã€‚
2. åªåœ¨è¨“ç·´é›†çš„æ–‡å­—ä¸Š `fit` å‘é‡å™¨ã€‚
3. æ¸¬è©¦é›†åªç”¨å‘é‡å™¨çš„ `transform` æ–¹æ³•è½‰æ›ï¼Œä¸å†é‡æ–° `fit`ã€‚

### âœ… é‡é»ç¨‹å¼ç¢¼

```python
from sklearn.model_selection import train_test_split

X_text_train, X_text_test, y_train, y_test = train_test_split(
    df['clean_text'], df['label'],
    test_size=0.2, random_state=9, stratify=df['label']
)

cv = CountVectorizer(max_features=10000)
X_train = cv.fit_transform(X_text_train)
X_test  = cv.transform(X_text_test)
```

## â„¹ï¸ 4. ä¿å­˜è™•ç†å¾Œçš„è³‡æ–™ï¼ˆArtifactsï¼‰

ç‚ºäº†æ–¹ä¾¿å¾ŒçºŒæ¨¡å‹è¨“ç·´èˆ‡éƒ¨ç½²ï¼Œå¯ä»¥å°‡å‘é‡å™¨èˆ‡åˆ‡åˆ†å¾Œçš„è³‡æ–™å­˜æˆæª”æ¡ˆï¼Œä¸‹æ¬¡ç›´æ¥è¼‰å…¥ä½¿ç”¨ã€‚

---

### âœ… é‡é»ç¨‹å¼ç¢¼

```python
import os, joblib
from scipy import sparse

os.makedirs("artifacts", exist_ok=True)
joblib.dump(cv, "artifacts/vectorizer.joblib")
sparse.save_npz("artifacts/X_train.npz", X_train)
sparse.save_npz("artifacts/X_test.npz",  X_test)
y_train.to_csv("artifacts/y_train.csv", index=False)
y_test.to_csv("artifacts/y_test.csv", index=False)
```

- ç”¢å‡ºç‰©ï¼š
    - `vectorizer.joblib`ï¼šå·²è¨“ç·´å¥½çš„å‘é‡å™¨ï¼ˆè©è¡¨èˆ‡è½‰æ›è¦å‰‡ï¼‰
    - `X_train.npz`ã€`X_test.npz`ï¼šå‘é‡åŒ–å¾Œçš„ç‰¹å¾µ
    - `y_train.csv`ã€`y_test.csv`ï¼šå°æ‡‰çš„æ¨™ç±¤


## âœ”ï¸ 5. é©—æ”¶æª¢æŸ¥æ¸…å–®

- â˜‘ï¸clean_text è™•ç†ç„¡ç¼ºå€¼
- â˜‘ï¸å…ˆåˆ‡åˆ†è³‡æ–™å†å‘é‡åŒ–
- â˜‘ï¸stratify ä¿æŒæ¨™ç±¤æ¯”ä¾‹ä¸€è‡´
- â˜‘ï¸å‘é‡å™¨åªåœ¨è¨“ç·´é›† fit
- â˜‘ï¸æ‰€æœ‰æª”æ¡ˆå·²æ­£ç¢ºå„²å­˜åˆ° artifacts/